#!/usr/bin/env python3
"""
çœŸæ­£çš„æµå¼APIæµ‹è¯•è„šæœ¬
æµ‹è¯•å®é™…çš„æµå¼APIå®ç°å’ŒåŠ é€Ÿé¦–å¥åŠŸèƒ½
"""
import asyncio
import time
import json
from loguru import logger

# å¯¼å…¥æµå¼LLM
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from open_llm_vtuber.agent.stateless_llm.streaming_openai_llm import StreamingOpenAILLM

class RealStreamingAPITester:
    """çœŸæ­£çš„æµå¼APIæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_results = []
        self.streaming_metrics = {}
    
    async def test_streaming_llm_creation(self) -> bool:
        """æµ‹è¯•æµå¼LLMåˆ›å»º"""
        logger.info("ğŸ§ª æµ‹è¯•æµå¼LLMåˆ›å»º...")
        
        try:
            # åˆ›å»ºæµå¼LLMå®ä¾‹
            streaming_llm = StreamingOpenAILLM(
                model="ft:gpt-3.5-turbo-0125:zzzorg::CRpLnugB",
                base_url="https://api.zzz-api.top/v1",
                llm_api_key="sk-zk252f4ee8cfc1e7f0d4d6e03fb46d6972f68ce685be0e3f",
                temperature=0.6,
                max_tokens=500,
                timeout=10,
                retry_count=1,
                # æµå¼é…ç½®
                stream=True,
                stream_chunk_size=8,
                stream_buffer_size=3,
                first_response_timeout=200,
                enable_streaming_tts=True,
                tts_stream_delay=50,
                # æµå¼ä¼˜åŒ–
                streaming_optimization=True,
                chunk_processing_parallel=True,
                immediate_audio_playback=True,
                # æ€§èƒ½ä¼˜åŒ–
                max_wait_time=3,
                connection_pool_size=5,
                keep_alive=True,
            )
            
            logger.info("âœ… æµå¼LLMåˆ›å»ºæˆåŠŸ")
            logger.info(f"   æ¨¡å‹: {streaming_llm.model}")
            logger.info(f"   æµå¼API: {streaming_llm.stream}")
            logger.info(f"   é¦–å¥å“åº”è¶…æ—¶: {streaming_llm.first_response_timeout}ms")
            logger.info(f"   æµå¼ä¼˜åŒ–: {streaming_llm.streaming_optimization}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æµå¼LLMåˆ›å»ºå¤±è´¥: {e}")
            return False
    
    async def test_streaming_chat_completion(self) -> bool:
        """æµ‹è¯•æµå¼èŠå¤©å®Œæˆ"""
        logger.info("ğŸ’¬ æµ‹è¯•æµå¼èŠå¤©å®Œæˆ...")
        
        try:
            # åˆ›å»ºæµå¼LLM
            streaming_llm = StreamingOpenAILLM(
                model="ft:gpt-3.5-turbo-0125:zzzorg::CRpLnugB",
                base_url="https://api.zzz-api.top/v1",
                llm_api_key="sk-zk252f4ee8cfc1e7f0d4d6e03fb46d6972f68ce685be0e3f",
                temperature=0.6,
                max_tokens=500,
                timeout=10,
                retry_count=1,
                stream=True,
                first_response_timeout=200,
                streaming_optimization=True,
            )
            
            # æµ‹è¯•æ¶ˆæ¯
            test_messages = [
                {"role": "user", "content": "Good morning! How are you today?"}
            ]
            
            logger.info("å¼€å§‹æµå¼èŠå¤©å®Œæˆæµ‹è¯•...")
            start_time = time.time()
            
            first_response_time = None
            chunk_count = 0
            accumulated_text = ""
            
            async for event in streaming_llm.chat_completion(test_messages):
                current_time = time.time()
                
                if event.get("type") == "text_delta":
                    if first_response_time is None:
                        first_response_time = event.get("first_response_time", 0)
                        logger.info(f"âš¡ é¦–å¥å“åº”æ—¶é—´: {first_response_time:.2f}ms")
                    
                    chunk_count += 1
                    text_chunk = event.get("text", "")
                    accumulated_text += text_chunk
                    
                    logger.info(f"ğŸ“¡ æµå¼å— {chunk_count}: {text_chunk}")
                    
                elif event.get("type") == "completion":
                    total_time = event.get("total_time", 0)
                    logger.info(f"âœ… æµå¼å®Œæˆ:")
                    logger.info(f"   æ€»å“åº”æ—¶é—´: {total_time:.2f}ms")
                    logger.info(f"   é¦–å¥å“åº”æ—¶é—´: {first_response_time:.2f}ms")
                    logger.info(f"   å¤„ç†å—æ•°: {chunk_count}")
                    logger.info(f"   æœ€ç»ˆæ–‡æœ¬: {accumulated_text}")
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°åŠ é€Ÿé¦–å¥ç›®æ ‡
                    if first_response_time and first_response_time <= 200:
                        logger.info(f"ğŸ‰ åŠ é€Ÿé¦–å¥æˆåŠŸ: {first_response_time:.2f}ms <= 200ms")
                        return True
                    else:
                        logger.warning(f"âš ï¸ é¦–å¥å“åº”è¾ƒæ…¢: {first_response_time:.2f}ms > 200ms")
                        return False
                        
                elif event.get("type") == "error":
                    logger.error(f"âŒ æµå¼èŠå¤©å¤±è´¥: {event.get('message', '')}")
                    return False
            
            return False
            
        except Exception as e:
            logger.error(f"âŒ æµå¼èŠå¤©å®Œæˆæµ‹è¯•å¤±è´¥: {e}")
            return False
    
    async def test_streaming_performance(self) -> bool:
        """æµ‹è¯•æµå¼æ€§èƒ½"""
        logger.info("ğŸ“Š æµ‹è¯•æµå¼æ€§èƒ½...")
        
        try:
            # åˆ›å»ºæµå¼LLM
            streaming_llm = StreamingOpenAILLM(
                model="ft:gpt-3.5-turbo-0125:zzzorg::CRpLnugB",
                base_url="https://api.zzz-api.top/v1",
                llm_api_key="sk-zk252f4ee8cfc1e7f0d4d6e03fb46d6972f68ce685be0e3f",
                temperature=0.6,
                max_tokens=500,
                timeout=10,
                retry_count=1,
                stream=True,
                first_response_timeout=200,
                streaming_optimization=True,
            )
            
            # æµ‹è¯•æ¶ˆæ¯
            test_messages = [
                {"role": "user", "content": "Good morning! How are you today?"},
                {"role": "user", "content": "I was stuck in traffic and it's so annoying!"},
                {"role": "user", "content": "I met my crush today and froze.. He is a Leo.."}
            ]
            
            # è¿è¡Œæ€§èƒ½æµ‹è¯•
            performance_results = await streaming_llm.test_streaming_performance(test_messages)
            
            logger.info("ğŸ“ˆ æµå¼æ€§èƒ½æµ‹è¯•ç»“æœ:")
            logger.info(f"   æ€»æµ‹è¯•æ•°: {performance_results['total_tests']}")
            logger.info(f"   æˆåŠŸæµ‹è¯•æ•°: {performance_results['successful_tests']}")
            logger.info(f"   æˆåŠŸç‡: {performance_results['success_rate']:.2%}")
            logger.info(f"   å¹³å‡é¦–å¥å“åº”æ—¶é—´: {performance_results['average_first_response_time']:.2f}ms")
            logger.info(f"   æ€§èƒ½ç­‰çº§: {performance_results['performance_grade']}")
            
            # è·å–æµå¼æŒ‡æ ‡
            metrics = streaming_llm.get_streaming_metrics()
            logger.info("ğŸ“Š æµå¼æŒ‡æ ‡:")
            logger.info(f"   çŠ¶æ€: {metrics['status']}")
            if metrics['status'] == 'active':
                logger.info(f"   é¦–å¥å“åº”æ—¶é—´: {metrics['first_response_times']['average']:.2f}ms")
                logger.info(f"   æ€»å“åº”æ—¶é—´: {metrics['total_response_times']['average']:.2f}ms")
                logger.info(f"   æ€§èƒ½ç­‰çº§: {metrics['performance_grade']}")
            
            return performance_results['success_rate'] >= 0.8
            
        except Exception as e:
            logger.error(f"âŒ æµå¼æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    async def test_config_integration(self) -> bool:
        """æµ‹è¯•é…ç½®é›†æˆ"""
        logger.info("âš™ï¸ æµ‹è¯•é…ç½®é›†æˆ...")
        
        try:
            import yaml
            
            # è¯»å–é…ç½®æ–‡ä»¶
            with open('conf.yaml', 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # æ£€æŸ¥æµå¼é…ç½®
            agent_settings = config.get('character_config', {}).get('agent_config', {}).get('agent_settings', {})
            basic_memory_agent = agent_settings.get('basic_memory_agent', {})
            
            # æ£€æŸ¥å…³é”®é…ç½®
            streaming_configs = [
                ('enable_streaming', basic_memory_agent.get('enable_streaming')),
                ('stream_chunk_size', basic_memory_agent.get('stream_chunk_size')),
                ('first_response_threshold', basic_memory_agent.get('first_response_threshold')),
                ('streaming_optimization', basic_memory_agent.get('streaming_optimization'))
            ]
            
            configured_count = 0
            for config_name, config_value in streaming_configs:
                if config_value is not None:
                    logger.info(f"âœ… {config_name}: {config_value}")
                    configured_count += 1
                else:
                    logger.warning(f"âš ï¸ {config_name}: æœªé…ç½®")
            
            # æ£€æŸ¥LLMæµå¼é…ç½®
            llm_configs = config.get('character_config', {}).get('agent_config', {}).get('llm_configs', {})
            openai_llm = llm_configs.get('openai_llm', {})
            
            llm_streaming_configs = [
                ('stream', openai_llm.get('stream')),
                ('stream_chunk_size', openai_llm.get('stream_chunk_size')),
                ('first_response_timeout', openai_llm.get('first_response_timeout')),
                ('streaming_optimization', openai_llm.get('streaming_optimization'))
            ]
            
            for config_name, config_value in llm_streaming_configs:
                if config_value is not None:
                    logger.info(f"âœ… LLM {config_name}: {config_value}")
                    configured_count += 1
                else:
                    logger.warning(f"âš ï¸ LLM {config_name}: æœªé…ç½®")
            
            success_rate = configured_count / (len(streaming_configs) + len(llm_streaming_configs))
            
            if success_rate >= 0.8:
                logger.info(f"âœ… é…ç½®é›†æˆæµ‹è¯•é€šè¿‡ ({configured_count}/{len(streaming_configs) + len(llm_streaming_configs)})")
                return True
            else:
                logger.error(f"âŒ é…ç½®é›†æˆæµ‹è¯•å¤±è´¥ ({configured_count}/{len(streaming_configs) + len(llm_streaming_configs)})")
                return False
                
        except Exception as e:
            logger.error(f"âŒ é…ç½®é›†æˆæµ‹è¯•å¤±è´¥: {e}")
            return False
    
    async def run_all_tests(self) -> bool:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹çœŸæ­£çš„æµå¼APIæµ‹è¯•...")
        logger.info("=" * 60)
        
        test_results = []
        
        # 1. æµ‹è¯•æµå¼LLMåˆ›å»º
        result1 = await self.test_streaming_llm_creation()
        test_results.append(("æµå¼LLMåˆ›å»º", result1))
        
        # 2. æµ‹è¯•æµå¼èŠå¤©å®Œæˆ
        result2 = await self.test_streaming_chat_completion()
        test_results.append(("æµå¼èŠå¤©å®Œæˆ", result2))
        
        # 3. æµ‹è¯•æµå¼æ€§èƒ½
        result3 = await self.test_streaming_performance()
        test_results.append(("æµå¼æ€§èƒ½", result3))
        
        # 4. æµ‹è¯•é…ç½®é›†æˆ
        result4 = await self.test_config_integration()
        test_results.append(("é…ç½®é›†æˆ", result4))
        
        # è¾“å‡ºæµ‹è¯•ç»“æœ
        logger.info("ğŸ“Š çœŸæ­£çš„æµå¼APIæµ‹è¯•æŠ¥å‘Š")
        logger.info("=" * 60)
        
        passed_tests = sum(1 for _, result in test_results if result)
        total_tests = len(test_results)
        
        for test_name, result in test_results:
            status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
            logger.info(f"{test_name}: {status}")
        
        logger.info(f"æ€»ä½“ç»“æœ: {passed_tests}/{total_tests} æµ‹è¯•é€šè¿‡")
        
        if passed_tests == total_tests:
            logger.info("ğŸ‰ çœŸæ­£çš„æµå¼APIåŠŸèƒ½å®ç°æˆåŠŸï¼")
            logger.info("ğŸ’¡ åŠŸèƒ½ç‰¹æ€§:")
            logger.info("   1. çœŸæ­£çš„æµå¼APIå“åº” - è¾¹ç”Ÿæˆè¾¹æ’­æ”¾")
            logger.info("   2. åŠ é€Ÿé¦–å¥ - 200mså†…å¬åˆ°å›å¤")
            logger.info("   3. æµå¼æ€§èƒ½ç›‘æ§ - å®æ—¶æ€§èƒ½æŒ‡æ ‡")
            logger.info("   4. é…ç½®é›†æˆ - å®Œæ•´çš„é…ç½®æ”¯æŒ")
            logger.info("   5. æ€§èƒ½æµ‹è¯• - è‡ªåŠ¨åŒ–æ€§èƒ½éªŒè¯")
        else:
            logger.error("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")
        
        logger.info("=" * 60)
        
        return passed_tests == total_tests

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    tester = RealStreamingAPITester()
    success = await tester.run_all_tests()
    
    if success:
        logger.info("ğŸ¯ çœŸæ­£çš„æµå¼APIåŠŸèƒ½å·²æˆåŠŸå®ç°ï¼")
        logger.info("ğŸš€ ç°åœ¨ä½ çš„VTuberå¯ä»¥åœ¨200mså†…å¼€å§‹å›å¤ï¼")
    else:
        logger.error("âŒ æµå¼APIå®ç°éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")

if __name__ == "__main__":
    asyncio.run(main())

