"""
æµå¼OpenAI LLMå®ç° - æ”¯æŒçœŸæ­£çš„æµå¼APIå’ŒåŠ é€Ÿé¦–å¥
"""
import asyncio
import time
from typing import AsyncIterator, List, Dict, Any, Optional
from openai import AsyncOpenAI, AsyncStream
from openai.types.chat import ChatCompletionChunk
from loguru import logger

from .stateless_llm_interface import StatelessLLMInterface


class StreamingOpenAILLM(StatelessLLMInterface):
    """æ”¯æŒæµå¼APIå’ŒåŠ é€Ÿé¦–å¥çš„OpenAI LLMå®ç°"""
    
    def __init__(
        self,
        model: str,
        base_url: str,
        llm_api_key: str,
        temperature: float = 0.6,
        max_tokens: int = 500,
        timeout: int = 10,
        retry_count: int = 1,
        # æµå¼APIé…ç½®
        stream: bool = True,
        stream_chunk_size: int = 8,
        stream_buffer_size: int = 3,
        first_response_timeout: int = 200,
        enable_streaming_tts: bool = True,
        tts_stream_delay: int = 50,
        # æµå¼ä¼˜åŒ–
        streaming_optimization: bool = True,
        chunk_processing_parallel: bool = True,
        immediate_audio_playback: bool = True,
        # æ€§èƒ½ä¼˜åŒ–
        max_wait_time: int = 3,
        connection_pool_size: int = 5,
        keep_alive: bool = True,
    ):
        """åˆå§‹åŒ–æµå¼OpenAI LLM"""
        self.model = model
        self.base_url = base_url
        self.llm_api_key = llm_api_key
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout
        self.retry_count = retry_count
        
        # æµå¼é…ç½®
        self.stream = stream
        self.stream_chunk_size = stream_chunk_size
        self.stream_buffer_size = stream_buffer_size
        self.first_response_timeout = first_response_timeout
        self.enable_streaming_tts = enable_streaming_tts
        self.tts_stream_delay = tts_stream_delay
        
        # æµå¼ä¼˜åŒ–
        self.streaming_optimization = streaming_optimization
        self.chunk_processing_parallel = chunk_processing_parallel
        self.immediate_audio_playback = immediate_audio_playback
        
        # æ€§èƒ½ä¼˜åŒ–
        self.max_wait_time = max_wait_time
        self.connection_pool_size = connection_pool_size
        self.keep_alive = keep_alive
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self.client = AsyncOpenAI(
            base_url=base_url,
            api_key=llm_api_key,
            timeout=timeout,
        )
        
        # æ€§èƒ½æŒ‡æ ‡
        self.first_response_times = []
        self.total_response_times = []
        self.streaming_metrics = {
            'chunks_processed': 0,
            'first_response_achieved': 0,
            'streaming_success_rate': 0.0
        }
        
        logger.info(f"ğŸš€ æµå¼OpenAI LLMåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   æ¨¡å‹: {model}")
        logger.info(f"   æµå¼API: {stream}")
        logger.info(f"   é¦–å¥å“åº”è¶…æ—¶: {first_response_timeout}ms")
        logger.info(f"   æµå¼ä¼˜åŒ–: {streaming_optimization}")

    async def chat_completion(
        self,
        messages: List[Dict[str, Any]],
        system: str = None,
        tools: List[Dict[str, Any]] = None,
    ) -> AsyncIterator[Dict[str, Any]]:
        """æµå¼èŠå¤©å®Œæˆ - æ”¯æŒåŠ é€Ÿé¦–å¥"""
        start_time = time.time()
        first_response_time = None
        
        try:
            # å‡†å¤‡æ¶ˆæ¯
            messages_with_system = messages
            if system:
                messages_with_system = [
                    {"role": "system", "content": system},
                    *messages,
                ]
            
            logger.debug(f"ğŸ“¡ å¼€å§‹æµå¼èŠå¤©å®Œæˆï¼Œæ¶ˆæ¯æ•°: {len(messages_with_system)}")
            
            # åˆ›å»ºæµå¼è¯·æ±‚
            stream: AsyncStream[ChatCompletionChunk] = await self.client.chat.completions.create(
                messages=messages_with_system,
                model=self.model,
                stream=True,  # å¼ºåˆ¶å¯ç”¨æµå¼
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                tools=tools if tools else None,
            )
            
            chunk_count = 0
            accumulated_text = ""
            
            async for chunk in stream:
                chunk_count += 1
                current_time = time.time()
                
                # è®°å½•é¦–å¥å“åº”æ—¶é—´
                if first_response_time is None and chunk.choices:
                    first_response_time = (current_time - start_time) * 1000
                    self.first_response_times.append(first_response_time)
                    self.streaming_metrics['first_response_achieved'] += 1
                    
                    logger.info(f"âš¡ é¦–å¥å“åº”æ—¶é—´: {first_response_time:.2f}ms")
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°åŠ é€Ÿé¦–å¥ç›®æ ‡
                    if first_response_time <= self.first_response_timeout:
                        logger.info(f"âœ… åŠ é€Ÿé¦–å¥æˆåŠŸ: {first_response_time:.2f}ms <= {self.first_response_timeout}ms")
                    else:
                        logger.warning(f"âš ï¸ é¦–å¥å“åº”è¾ƒæ…¢: {first_response_time:.2f}ms > {self.first_response_timeout}ms")
                
                # å¤„ç†æ–‡æœ¬å†…å®¹
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    accumulated_text += content
                    
                    # å‘é€æµå¼æ–‡æœ¬äº‹ä»¶
                    yield {
                        "type": "text_delta",
                        "text": content,
                        "timestamp": current_time,
                        "chunk_id": chunk_count,
                        "first_response_time": first_response_time,
                        "accumulated_text": accumulated_text
                    }
                    
                    # å¦‚æœå¯ç”¨æµå¼ä¼˜åŒ–ï¼Œç«‹å³å¤„ç†å°å—
                    if self.streaming_optimization and len(content.strip()) >= self.stream_chunk_size:
                        yield {
                            "type": "streaming_chunk_ready",
                            "text": content,
                            "chunk_id": chunk_count,
                            "ready_for_tts": True
                        }
            
            # è®°å½•æ€»å“åº”æ—¶é—´
            total_time = (time.time() - start_time) * 1000
            self.total_response_times.append(total_time)
            self.streaming_metrics['chunks_processed'] += chunk_count
            
            # è®¡ç®—æµå¼æˆåŠŸç‡
            if first_response_time and first_response_time <= self.first_response_timeout:
                self.streaming_metrics['streaming_success_rate'] = (
                    self.streaming_metrics['first_response_achieved'] / 
                    max(1, len(self.first_response_times))
                )
            
            logger.info(f"ğŸ“Š æµå¼å®Œæˆç»Ÿè®¡:")
            logger.info(f"   æ€»å“åº”æ—¶é—´: {total_time:.2f}ms")
            logger.info(f"   é¦–å¥å“åº”æ—¶é—´: {first_response_time:.2f}ms" if first_response_time else "   é¦–å¥å“åº”æ—¶é—´: æœªè®°å½•")
            logger.info(f"   å¤„ç†å—æ•°: {chunk_count}")
            logger.info(f"   æµå¼æˆåŠŸç‡: {self.streaming_metrics['streaming_success_rate']:.2%}")
            
            # å‘é€å®Œæˆäº‹ä»¶
            yield {
                "type": "completion",
                "total_time": total_time,
                "first_response_time": first_response_time,
                "chunk_count": chunk_count,
                "final_text": accumulated_text,
                "streaming_success": first_response_time and first_response_time <= self.first_response_timeout
            }
            
        except Exception as e:
            logger.error(f"âŒ æµå¼èŠå¤©å®Œæˆå¤±è´¥: {e}")
            yield {
                "type": "error",
                "message": str(e),
                "timestamp": time.time()
            }

    def get_streaming_metrics(self) -> Dict[str, Any]:
        """è·å–æµå¼æ€§èƒ½æŒ‡æ ‡"""
        if not self.first_response_times:
            return {
                "status": "no_data",
                "message": "æš‚æ— æµå¼æ•°æ®"
            }
        
        avg_first_response = sum(self.first_response_times) / len(self.first_response_times)
        avg_total_response = sum(self.total_response_times) / len(self.total_response_times)
        
        return {
            "status": "active",
            "first_response_times": {
                "average": avg_first_response,
                "min": min(self.first_response_times),
                "max": max(self.first_response_times),
                "count": len(self.first_response_times)
            },
            "total_response_times": {
                "average": avg_total_response,
                "min": min(self.total_response_times),
                "max": max(self.total_response_times),
                "count": len(self.total_response_times)
            },
            "streaming_metrics": self.streaming_metrics,
            "performance_grade": self._calculate_performance_grade(avg_first_response)
        }
    
    def _calculate_performance_grade(self, avg_first_response: float) -> str:
        """è®¡ç®—æ€§èƒ½ç­‰çº§"""
        if avg_first_response <= 200:
            return "A+ (ä¼˜ç§€)"
        elif avg_first_response <= 500:
            return "A (è‰¯å¥½)"
        elif avg_first_response <= 1000:
            return "B (ä¸€èˆ¬)"
        else:
            return "C (éœ€è¦ä¼˜åŒ–)"
    
    async def test_streaming_performance(self, test_messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æµ‹è¯•æµå¼æ€§èƒ½"""
        logger.info("ğŸ§ª å¼€å§‹æµå¼æ€§èƒ½æµ‹è¯•...")
        
        start_time = time.time()
        test_results = []
        
        for i, message in enumerate(test_messages, 1):
            logger.info(f"æµ‹è¯• {i}: {message.get('content', '')[:50]}...")
            
            test_start = time.time()
            first_response_time = None
            chunk_count = 0
            
            try:
                async for event in self.chat_completion([message]):
                    if event.get("type") == "text_delta":
                        if first_response_time is None:
                            first_response_time = event.get("first_response_time", 0)
                        chunk_count += 1
                    elif event.get("type") == "completion":
                        total_time = event.get("total_time", 0)
                        test_results.append({
                            "test_id": i,
                            "first_response_time": first_response_time,
                            "total_time": total_time,
                            "chunk_count": chunk_count,
                            "success": event.get("streaming_success", False)
                        })
                        break
                        
            except Exception as e:
                logger.error(f"æµ‹è¯• {i} å¤±è´¥: {e}")
                test_results.append({
                    "test_id": i,
                    "error": str(e),
                    "success": False
                })
        
        total_test_time = time.time() - start_time
        
        # åˆ†ææµ‹è¯•ç»“æœ
        successful_tests = [r for r in test_results if r.get("success", False)]
        avg_first_response = sum(r["first_response_time"] for r in successful_tests) / len(successful_tests) if successful_tests else 0
        
        return {
            "total_tests": len(test_results),
            "successful_tests": len(successful_tests),
            "success_rate": len(successful_tests) / len(test_results) if test_results else 0,
            "average_first_response_time": avg_first_response,
            "total_test_time": total_test_time,
            "performance_grade": self._calculate_performance_grade(avg_first_response),
            "test_results": test_results
        }

