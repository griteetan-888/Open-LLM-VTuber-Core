"""
è¶…å¿«é€Ÿæµå¼å¤„ç†ç³»ç»Ÿ - å®ç°2-3ç§’å“åº”æ—¶é—´
"""
import asyncio
import time
import json
import hashlib
from typing import AsyncIterator, Dict, Any, Optional, List
from loguru import logger
from pathlib import Path
import aiofiles

class UltraFastStreamingProcessor:
    """è¶…å¿«é€Ÿæµå¼å¤„ç†å™¨ - ä¸“é—¨ä¸º2-3ç§’å“åº”æ—¶é—´ä¼˜åŒ–"""
    
    def __init__(self, cache_dir: str = "cache/ultra_fast"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # é¢„è®¡ç®—ç¼“å­˜
        self.precomputed_responses = {}
        self.common_phrases_cache = {}
        
        # æ€§èƒ½æŒ‡æ ‡
        self.response_times = []
        self.cache_hit_rate = 0.0
        
        # è¶…å¿«é€Ÿé…ç½®
        self.max_response_time = 3.0  # æœ€å¤§å“åº”æ—¶é—´3ç§’
        self.chunk_size = 5           # è¶…å°å—å¤„ç†
        self.parallel_limit = 5       # å¹¶è¡Œå¤„ç†é™åˆ¶
        
    async def process_ultra_fast_response(
        self, 
        user_input: str, 
        llm_engine, 
        tts_engine,
        websocket_send
    ) -> str:
        """è¶…å¿«é€Ÿå“åº”å¤„ç† - ç›®æ ‡2-3ç§’"""
        start_time = time.time()
        
        # 1. æ£€æŸ¥é¢„è®¡ç®—ç¼“å­˜ (0.1ç§’å†…)
        cached_response = await self._check_precomputed_cache(user_input)
        if cached_response:
            logger.info("âš¡ ä½¿ç”¨é¢„è®¡ç®—ç¼“å­˜å“åº”")
            await self._send_cached_response(cached_response, websocket_send)
            return cached_response["text"]
        
        # 2. å¹¶è¡Œå¯åŠ¨LLMå’ŒTTSå‡†å¤‡ (0.2ç§’å†…)
        llm_task = asyncio.create_task(
            self._ultra_fast_llm_generation(user_input, llm_engine)
        )
        
        # 3. æµå¼å¤„ç†LLMå“åº” (1-2ç§’å†…)
        response_text = ""
        tts_tasks = []
        
        async for chunk in self._stream_llm_response(llm_task):
            if chunk:
                response_text += chunk
                
                # ç«‹å³å‘é€æ–‡æœ¬åˆ°å‰ç«¯
                await websocket_send(json.dumps({
                    "type": "text-stream",
                    "text": chunk,
                    "timestamp": time.time()
                }))
                
                # å¹¶è¡Œå¯åŠ¨TTSç”Ÿæˆ
                if len(chunk.strip()) > 3:  # åªå¯¹æœ‰æ„ä¹‰çš„å†…å®¹ç”ŸæˆTTS
                    tts_task = asyncio.create_task(
                        self._ultra_fast_tts_generation(chunk, tts_engine)
                    )
                    tts_tasks.append(tts_task)
        
        # 4. å¹¶è¡Œå¤„ç†æ‰€æœ‰TTSä»»åŠ¡ (1ç§’å†…)
        if tts_tasks:
            await self._process_parallel_tts(tts_tasks, websocket_send)
        
        # 5. ç¼“å­˜å“åº”
        await self._cache_response(user_input, response_text)
        
        total_time = time.time() - start_time
        self.response_times.append(total_time)
        
        logger.info(f"âš¡ è¶…å¿«é€Ÿå“åº”å®Œæˆ: {total_time:.2f}ç§’")
        
        if total_time > self.max_response_time:
            logger.warning(f"âš ï¸ å“åº”æ—¶é—´è¶…è¿‡ç›®æ ‡: {total_time:.2f}s > {self.max_response_time}s")
        
        return response_text
    
    async def _check_precomputed_cache(self, user_input: str) -> Optional[Dict]:
        """æ£€æŸ¥é¢„è®¡ç®—ç¼“å­˜"""
        cache_key = hashlib.md5(user_input.lower().strip().encode()).hexdigest()
        cache_file = self.cache_dir / f"{cache_key}.json"
        
        if cache_file.exists():
            try:
                async with aiofiles.open(cache_file, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    return json.loads(content)
            except Exception as e:
                logger.warning(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}")
        
        return None
    
    async def _ultra_fast_llm_generation(self, user_input: str, llm_engine) -> AsyncIterator[str]:
        """è¶…å¿«é€ŸLLMç”Ÿæˆ"""
        try:
            # ä½¿ç”¨æœ€å°åŒ–é…ç½®
            messages = [
                {"role": "user", "content": user_input}
            ]
            
            # æµå¼ç”Ÿæˆï¼Œå°å—å¤„ç†
            async for chunk in llm_engine.chat_completion(messages, stream=True):
                if chunk and len(chunk.strip()) > 0:
                    yield chunk
                    
        except Exception as e:
            logger.error(f"LLMç”Ÿæˆå¤±è´¥: {e}")
            yield "Sorry, I need a moment to think..."
    
    async def _stream_llm_response(self, llm_task) -> AsyncIterator[str]:
        """æµå¼å¤„ç†LLMå“åº”"""
        try:
            # ç­‰å¾…LLMä»»åŠ¡å®Œæˆï¼Œä½†è®¾ç½®è¶…æ—¶
            response = await asyncio.wait_for(llm_task, timeout=self.max_response_time)
            
            # å¦‚æœLLMè¿”å›å®Œæ•´å“åº”ï¼Œåˆ™åˆ†å—å‘é€
            if isinstance(response, str):
                words = response.split()
                for i in range(0, len(words), self.chunk_size):
                    chunk = " ".join(words[i:i+self.chunk_size])
                    if chunk.strip():
                        yield chunk
                        await asyncio.sleep(0.05)  # çŸ­æš‚å»¶è¿Ÿä»¥æ¨¡æ‹Ÿæµå¼æ•ˆæœ
            else:
                # å¦‚æœå·²ç»æ˜¯æµå¼å“åº”
                async for chunk in response:
                    yield chunk
                    
        except asyncio.TimeoutError:
            logger.warning("LLMå“åº”è¶…æ—¶ï¼Œä½¿ç”¨é»˜è®¤å“åº”")
            yield "Let me think about that..."
        except Exception as e:
            logger.error(f"æµå¼å¤„ç†å¤±è´¥: {e}")
            yield "I'm processing your request..."
    
    async def _ultra_fast_tts_generation(self, text: str, tts_engine) -> Optional[str]:
        """è¶…å¿«é€ŸTTSç”Ÿæˆ"""
        try:
            # æ£€æŸ¥TTSç¼“å­˜
            tts_cache_key = hashlib.md5(text.encode()).hexdigest()
            cached_audio = self.cache_dir / f"tts_{tts_cache_key}.mp3"
            
            if cached_audio.exists():
                return str(cached_audio)
            
            # ç”Ÿæˆæ–°éŸ³é¢‘
            audio_path = await tts_engine.async_generate_audio(
                text=text,
                file_name_no_ext=f"ultra_fast_{int(time.time())}"
            )
            
            # ç¼“å­˜éŸ³é¢‘
            if audio_path:
                import shutil
                shutil.copy2(audio_path, cached_audio)
            
            return audio_path
            
        except Exception as e:
            logger.error(f"TTSç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    async def _process_parallel_tts(self, tts_tasks: List[asyncio.Task], websocket_send):
        """å¹¶è¡Œå¤„ç†TTSä»»åŠ¡"""
        try:
            # ç­‰å¾…æ‰€æœ‰TTSä»»åŠ¡å®Œæˆ
            results = await asyncio.gather(*tts_tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if isinstance(result, str) and result:
                    # å‘é€éŸ³é¢‘åˆ°å‰ç«¯
                    await websocket_send(json.dumps({
                        "type": "audio",
                        "audio_path": result,
                        "sequence": i,
                        "timestamp": time.time()
                    }))
                    
        except Exception as e:
            logger.error(f"å¹¶è¡ŒTTSå¤„ç†å¤±è´¥: {e}")
    
    async def _send_cached_response(self, cached_data: Dict, websocket_send):
        """å‘é€ç¼“å­˜å“åº”"""
        try:
            # å‘é€æ–‡æœ¬
            await websocket_send(json.dumps({
                "type": "cached-text",
                "text": cached_data["text"],
                "timestamp": time.time()
            }))
            
            # å‘é€éŸ³é¢‘
            if cached_data.get("audio_path"):
                await websocket_send(json.dumps({
                    "type": "cached-audio",
                    "audio_path": cached_data["audio_path"],
                    "timestamp": time.time()
                }))
                
        except Exception as e:
            logger.error(f"å‘é€ç¼“å­˜å“åº”å¤±è´¥: {e}")
    
    async def _cache_response(self, user_input: str, response_text: str):
        """ç¼“å­˜å“åº”"""
        try:
            cache_key = hashlib.md5(user_input.lower().strip().encode()).hexdigest()
            cache_data = {
                "text": response_text,
                "timestamp": time.time(),
                "user_input": user_input
            }
            
            cache_file = self.cache_dir / f"{cache_key}.json"
            async with aiofiles.open(cache_file, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(cache_data, ensure_ascii=False, indent=2))
                
        except Exception as e:
            logger.error(f"ç¼“å­˜å“åº”å¤±è´¥: {e}")
    
    def precompute_common_responses(self):
        """é¢„è®¡ç®—å¸¸è§å“åº”"""
        common_inputs = [
            "Hello", "Hi", "How are you?", "What's up?", "Good morning",
            "Good afternoon", "Good evening", "Thank you", "Thanks",
            "You're welcome", "Nice to meet you", "See you later"
        ]
        
        logger.info("ğŸ”„ é¢„è®¡ç®—å¸¸è§å“åº”...")
        # è¿™é‡Œå¯ä»¥é¢„å…ˆç”Ÿæˆå¸¸è§å“åº”çš„ç¼“å­˜
        # å®é™…å®ç°éœ€è¦æ ¹æ®å…·ä½“çš„LLMå’ŒTTSå¼•æ“æ¥è°ƒæ•´
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if not self.response_times:
            return {"error": "no_data"}
        
        avg_time = sum(self.response_times) / len(self.response_times)
        max_time = max(self.response_times)
        min_time = min(self.response_times)
        
        success_rate = len([t for t in self.response_times if t <= self.max_response_time]) / len(self.response_times)
        
        return {
            "average_response_time": avg_time,
            "max_response_time": max_time,
            "min_response_time": min_time,
            "success_rate": success_rate,
            "target_achieved": success_rate >= 0.8,  # 80%çš„è¯·æ±‚åœ¨ç›®æ ‡æ—¶é—´å†…å®Œæˆ
            "total_requests": len(self.response_times)
        }

class UltraFastTTSManager:
    """è¶…å¿«é€ŸTTSç®¡ç†å™¨"""
    
    def __init__(self):
        self.audio_cache = {}
        self.preloaded_phrases = {}
        self.parallel_semaphore = asyncio.Semaphore(3)  # é™åˆ¶å¹¶è¡ŒTTSä»»åŠ¡
    
    async def generate_audio_ultra_fast(self, text: str, tts_engine) -> Optional[str]:
        """è¶…å¿«é€ŸéŸ³é¢‘ç”Ÿæˆ"""
        async with self.parallel_semaphore:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = hashlib.md5(text.encode()).hexdigest()
            if cache_key in self.audio_cache:
                return self.audio_cache[cache_key]
            
            try:
                # ç”ŸæˆéŸ³é¢‘
                audio_path = await tts_engine.async_generate_audio(
                    text=text,
                    file_name_no_ext=f"ultra_fast_{cache_key[:8]}"
                )
                
                # ç¼“å­˜ç»“æœ
                if audio_path:
                    self.audio_cache[cache_key] = audio_path
                
                return audio_path
                
            except Exception as e:
                logger.error(f"è¶…å¿«é€ŸTTSç”Ÿæˆå¤±è´¥: {e}")
                return None
    
    def preload_common_phrases(self, phrases: List[str], tts_engine):
        """é¢„åŠ è½½å¸¸è§çŸ­è¯­"""
        logger.info(f"ğŸ”„ é¢„åŠ è½½ {len(phrases)} ä¸ªå¸¸è§çŸ­è¯­...")
        
        async def _preload():
            tasks = []
            for phrase in phrases:
                task = asyncio.create_task(
                    self.generate_audio_ultra_fast(phrase, tts_engine)
                )
                tasks.append(task)
            
            await asyncio.gather(*tasks, return_exceptions=True)
            logger.info("âœ… å¸¸è§çŸ­è¯­é¢„åŠ è½½å®Œæˆ")
        
        # åœ¨åå°è¿è¡Œé¢„åŠ è½½
        asyncio.create_task(_preload())
